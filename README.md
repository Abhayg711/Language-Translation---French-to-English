# Language-Translation---French-to-English


Objective - Trying to implement language translation i.e. context from one language to another language in this program I am trying to convert French to English using seq-to-seq model in this case various variations of BERT Transfomers.

Conclusion - "ROBERTA" gave best accuracy for both fine-tuned & without fine-tuned model.

Introduction to BERT Transformers:-
Bidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training. The key innovation of the BERT model lies in applying the bidirectional training of Transformer models to language modeling.

As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads it bidirectionally, meaning the entire sequence of words at once. The results demonstrated by the BERT model show that a bidirectionally trained language model can have a deeper sense of language context and flow than single-direction language models.

Detailed understanding of BERT Transfomer - https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270; http://jalammar.github.io/illustrated-bert/
